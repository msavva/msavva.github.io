<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <!-- Use the .htaccess and remove these lines to avoid edge case issues.
     More info: h5bp.com/i/378 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

  <!-- Our site title and description -->
  <title>Manolis Savva | Manolis Savva</title>

  <meta name="generator" content="DocPad v6.79.4" />

  <!-- Mobile viewport optimized: h5bp.com/viewport -->
  <meta name="viewport" content="width=device-width" />

  <!-- Shims: IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script async src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <style >html.wait {
	cursor: wait !important;
	opacity: 0;
	transition: opacity 0.5s ease;
}</style><link  rel="stylesheet" href="/styles/twitter-bootstrap.css" /><link  rel="stylesheet" href="/styles/style.css" />
  <script >(function(){
	/* Did we just livereload? */
var log = !!(localStorage && console && console.log && true);
if ( log && localStorage.getItem('/docpad-livereload/reloaded') === 'yes' ) {
	localStorage.removeItem('/docpad-livereload/reloaded');
	console.log('LiveReload completed at', new Date())
}

/* Listen for the regenerated event and perform a reload of the page when the event occurs */
var listen = function(){
	var primus = new Primus('/docpad-livereload');
	primus.on('data', function(data){
		if ( data && data.message ) {
			if ( data.message === 'generateBefore' ) {
				if ( log ) {
					console.log('LiveReload started at', new Date());
				}
				if ( typeof document.getElementsByTagName !== 'undefined' ) {
	document.getElementsByTagName('html')[0].className += ' wait';
}
			}
			else if ( data.message === 'generateAfter' ) {
				if ( log ) {
					localStorage.setItem('/docpad-livereload/reloaded', 'yes');
				}
				document.location.reload();
			}
		}
	});
};
	/* Inject socket into our page */
var inject = function(){
	var t = document.createElement('script');
	t.type = 'text/javascript';
	t.async = 'async';
	t.src = '/docpad-livereload/primus.js';
	t.onload = listen;
	var s = document.getElementsByTagName('script')[0];
	s.parentNode.insertBefore(t, s);
};
	if ( typeof Primus !== 'undefined' ) {
		listen();
	} else {
		inject();
	}
})();</script><script  src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script  src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script><script  src="/scripts/bootstrap.min.js"></script><script  src="/scripts/script.js"></script>
</head>
<body>
  <div class="container">
    <section id="content" class="content">
      <div class="row">
        <div class="col-xs-12">
          <h1><a href="index.html">Manolis Savva</a></h1>
        </div>
      </div>
      <hr>
      <div class="row">
  <div class="col-xs-3">
    <div style="margin-bottom:0.1cm;"><img class="img img-responsive" src="files/photo.jpg" alt="Manolis Savva" width="250px"/></div>
<div>Associate Professor</div>
<div><a href="https://www.sfu.ca/">Simon Fraser University</a></div>
<div><a href="https://cs.sfu.ca/">School of Computing Science</a></div>
<div style="margin-bottom:0.3cm;"><a href="https://3dlg-hcvc.github.io">3DLG</a> | <a href="https://gruvi.cs.sfu.ca//">GrUVi</a> | <a href="http://ml.cs.sfu.ca/">SFU AI/ML</a> | <a href="https://vinci.sfu.ca">VINCI</a></div>
<div style="margin-bottom:0.3cm;"><a href='https://www.chairs-chaires.gc.ca/home-accueil-eng.aspx'>Canada Research Chair</a> in<br>Computer Graphics</div>
<!-- <div>8888 University Dr., Burnaby BC, Canada</div> -->
<div style="margin-bottom:0.3cm;"><a href="files/CV.pdf">CV/R&eacute;sum&eacute;</a> (updated 2018-10-13)</div>
<a href="https://github.com/msavva" target="_blank"><i class="fa fa-github-square fa-3x"></i></a>
&nbsp;
<a href="mailto:msavva@sfu.ca"><i class="fa fa-envelope-square fa-3x"></i></a>
&nbsp;
<a href="https://scholar.google.com/citations?user=4D2vsdYAAAAJ" target="_blank"><i class="fa fa-graduation-cap fa-3x"></i></a>
&nbsp;
    <h3>Teaching</h3>
<hr>
<strong>Spring 2025</strong><br>
<a href="https://canvas.sfu.ca/courses/87505">CMPT722: Rendering & Visual Computing for AI</a><br>
<strong>Spring 2024</strong><br>
<a href="https://canvas.sfu.ca/courses/81598">CMPT722: Rendering & Visual Computing for AI</a><br>
<a href="https://canvas.sfu.ca/courses/81599">CMPT757: Frontiers of Visual Computing</a><br>
<strong>Spring 2023</strong><br>
<a href="https://canvas.sfu.ca/courses/75004">CMPT700: Technical Writing & Research Comm.</a><br>
<strong>Fall 2022</strong><br>
<a href="https://canvas.sfu.ca/courses/72688">CMPT985: Rendering & Visual Computing for AI</a><br>
<strong>Spring 2022</strong><br>
<a href="https://canvas.sfu.ca/courses/68030">CMPT700: Technical Writing & Research Comm.</a><br>
<a href="http://yaksoy.github.io/introvc/">CMPT361: Intro to Visual Computing</a><br>
<strong>Fall 2021</strong><br>
<a href="http://yaksoy.github.io/introvc/">CMPT361: Intro to Visual Computing</a><br>
<strong>Spring 2021</strong><br>
<a href="https://canvas.sfu.ca/courses/59537">CMPT700: Technical Writing & Research Comm.</a><br>
<strong>Fall 2019 & Fall 2020</strong><br>
<a href="https://msavva.github.io/cmpt757/">CMPT757: Frontiers of Visual Computing</a><br>
<strong>Summer 2015 (Stanford)</strong><br>
<a href="http://web.stanford.edu/class/cs148/">CS148: Intro to Comp. Graphics & Imaging</a>
    




<h3>Students & Visitors</h3>
<hr>

  <a href="https://www.sfu.ca/~dya78/">Dongchen Yang</a> <br>

  <a href="https://houip.github.io/portfolio/">Hou In (Derek) Pun</a> <br>

  <a href="https://iv-t.github.io/">Hou In (Ivan) Tam</a> <br>

  <a href="https://sevenljy.github.io/">Jiayi Liu</a> (co-advised with Ali Mahdavi-Amiri)<br>

  <a href="https://reza-asad.github.io/">Reza Asad</a> (co-advised with Sharan Vaswani)<br>

  <a href="">Weikun Peng</a> <br>


<h3>Alumni</h3>
<hr>

  <a href="https://www.linkedin.com/in/armin-kavian/">Armin Kavian</a> (next: CC&amp;L)<br>

  <a href="https://changan.io/">Changan Chen</a> (next: PhD at UT Austin)<br>

  <a href="https://christinatan0704.github.io/mysite/">Jiaqi Tan</a> (next: PhD at SFU)<br>

  <a href="https://madhawav.github.io/">Madhawa Vidanapathirana</a> (next: Microsoft)<br>

  <a href="https://www.sanjayharesh.com/">Sanjay Haresh</a> (next: Qualcomm AI)<br>

  <a href="https://shivanshpatel35.github.io/">Shivansh Patel</a> (next: PhD at UIUC)<br>

  <a href="https://supriya-gdptl.github.io/">Supriya Gadi Patil</a><br>

  <a href="https://sammaoys.github.io/">Yongsen Mao</a> (next: KuJiaLe AI)<br>


  </div>
  <div class="col-xs-9">
    <p>
  I am an Associate Professor in the <a href="https://cs.sfu.ca"></a>School of Computing Science</a> at <a href="https://www.sfu.ca">Simon Fraser University</a>, and <a href='https://www.chairs-chaires.gc.ca/home-accueil-eng.aspx'>Canada Research Chair</a> in Computer Graphics.
  My research focuses on analysis, organization and generation of 3D content.
  The methods that I work on are stepping stones towards holistic 3D scene understanding revolving around people, with applications in computer graphics, computer vision, and robotics.
</p>
<p>
  Prior to my current position I was a visiting researcher at <a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research</a> and a postdoctoral research associate at the Princeton University <a href="https://gfx.cs.princeton.edu/">Computer Graphics</a> and <a href="https://vision.princeton.edu/">Vision</a> Labs.
  I received my Ph.D. from <a href="https://www.stanford.edu">Stanford University</a>, under the supervision of <a href="https://graphics.stanford.edu/~hanrahan">Pat Hanrahan</a>.
  My undergraduate degree was in Physics and Computer Science at <a href="https://www.cornell.edu">Cornell University</a>.
</p>
<!-- <h3>Research Interests</h3>
<ul>
  <li>Human-centric 3D scene analysis</li>
  <li>3D scene synthesis for VR/AR content creation, and learning through simulation</li>
  <li>Connecting natural language with 3D representations</li>
  <li>Data visualization</li>
</ul> -->

    <h3>News</h3>
<hr>
<p>

  
  
    <div class="row">
      <div class="col-xs-2">Mar 2025</div>
      <div class="col-xs-10">One paper at ICLR 2025, one at 3DV 2025 (oral), and one Eurographics 2025 STAR. Congratulations to Jiayi, Denys, Ivan, Derek, and Austin! Also, congratulations to Ivan for defending his MSc thesis and starting as a PhD student in the group.</div>
    </div>
  
    <div class="row">
      <div class="col-xs-2">May 2024</div>
      <div class="col-xs-10">Tenured and promoted to Associate Professor! I am grateful for all the support by my students, colleagues, and collaborators.</div>
    </div>
  
    <div class="row">
      <div class="col-xs-2">Mar 2024</div>
      <div class="col-xs-10">Three papers at CVPR 2024, two at 3DV 2024, one at WACV 2024, and one Eurographics 2024 STAR.<br>Congratulations to Jiayi, Ivan, Mukul, Yongsen, Sanjay, Qirui, Xiahao, Hanxiao, Han-Hung, and Sonia!</div>
    </div>
  
    <div class="row">
      <div class="col-xs-2">Jul 2023</div>
      <div class="col-xs-10">New paper on articulated object reconstruction at ICCV 2023. Congratulations to Jiayi!</div>
    </div>
  
    <div class="row">
      <div class="col-xs-2">May 2023</div>
      <div class="col-xs-10">Congratulations to Sanjay and Supriya for defending their MSc theses.<br>"Emergence of Maps in the Memories of Blind Navigation Agents" receives ICLR 2023 Outstanding Paper Award.</div>
    </div>
  
    <div class="row">
      <div class="col-xs-2">May 2022</div>
      <div class="col-xs-10">I am honored to receive a <a href='https://graphicsinterface.org/awards/early-career-researcher-award/manolis-savva/'>CHCCS Early Career Researcher Award</a>.</div>
    </div>
  
    <div class="row">
      <div class="col-xs-2">Jan 2022</div>
      <div class="col-xs-10">Congratulations to Jiaqi for defending her MSc thesis!</div>
    </div>
  
    <div class="row">
      <div class="col-xs-2">Oct 2021</div>
      <div class="col-xs-10">Habitat 2.0 to be presented at NeurIPS 2021 as a spotlight! Check out the project at <a href='https://aihabitat.org'>aihabitat.org</a>.</div>
    </div>
  
  <a href="news.html">More...</a>

</p>

    <h3>Publications</h3>
<hr>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/diorama/"><img src="files/diorama.webp" alt="Diorama: Unleashing Zero-shot Single-view 3D Scene Modeling" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/diorama/" class="paper-title">Diorama: Unleashing Zero-shot Single-view 3D Scene Modeling</a><br>
      <a href="https://ca.linkedin.com/in/qirui-wu-946450bb">Qirui Wu</a>, <a href="https://scholar.google.com/citations?user=guExFlYAAAAJ">Denys Iliash</a>, <a href="https://dritchie.github.io/">Daniel Ritchie</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a><br>
      ICCV 2025<br>
      <a href="https://arxiv.org/pdf/2411.19492">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/diorama/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://arxiv.org/abs/2403.14937"><img src="files/artstar.png" alt="Survey on Modeling of Human-made Articulated Objects" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://arxiv.org/abs/2403.14937" class="paper-title">Survey on Modeling of Human-made Articulated Objects</a><br>
      <a href="https://sevenljy.github.io/">Jiayi Liu</a>, Manolis Savva, <a href="https://www.sfu.ca/~amahdavi/">Ali Mahdavi-Amiri</a><br>
      Eurographics STAR (State of The Art Report), CGF 2025<br>
      <a href="https://arxiv.org/pdf/2403.14937">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://arxiv.org/abs/2403.14937">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/singapo/"><img src="files/singapo.png" alt="SINGAPO: Single Image Controlled Generation of Articulated Parts in Objects" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/singapo/" class="paper-title">SINGAPO: Single Image Controlled Generation of Articulated Parts in Objects</a><br>
      <a href="https://sevenljy.github.io/">Jiayi Liu</a>, <a href="https://scholar.google.com/citations?user=guExFlYAAAAJ">Denys Iliash</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="https://www.sfu.ca/~amahdavi/">Ali Mahdavi-Amiri</a><br>
      ICLR 2025<br>
      <a href="https://arxiv.org/pdf/2410.16499">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/singapo/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/smc/"><img src="files/smc.png" alt="SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/smc/" class="paper-title">SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements</a><br>
      <a href="https://iv-t.github.io/">Hou In (Ivan) Tam</a>, <a href="https://houip.github.io/portfolio/">Hou In (Derek) Pun</a>, <a href="https://atwang16.github.io/">Austin T. Wang</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva<br>
      3DV 2025<br>
      <a href="https://arxiv.org/pdf/2408.02211">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/smc/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/r3ds/"><img src="files/r3ds.webp" alt="R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/r3ds/" class="paper-title">R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding</a><br>
      <a href="https://ca.linkedin.com/in/qirui-wu-946450bb">Qirui Wu</a>, <a href="https://www.linkedin.com/in/sonia-raychaudhuri/">Sonia Raychaudhuri</a>, <a href="https://dritchie.github.io/">Daniel Ritchie</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a><br>
      ECCV 2024<br>
      <a href="https://arxiv.org/pdf/2403.12301">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/r3ds/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://github.com/mmahdavian/DMFuser"><img src="files/dmfuser.png" alt="DMFuser: Distilled Multi-Task Learning for End-to-end Transformer-Based Sensor Fusion in Autonomous Driving" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://github.com/mmahdavian/DMFuser" class="paper-title">DMFuser: Distilled Multi-Task Learning for End-to-end Transformer-Based Sensor Fusion in Autonomous Driving</a><br>
      <a href="https://scholar.google.com/citations?user=URfHnY4AAAAJ&hl=en">Pedram Agand</a>, <a href="https://scholar.google.com/citations?user=fhTJ8m4AAAAJ&hl=en">Mohammad Mahdavian</a>, Manolis Savva, <a href="https://sfumars.com/people/">Mo Chen</a><br>
      IROS 2024<br>
      <a href="https://ieeexplore.ieee.org/document/10802740">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://github.com/mmahdavian/DMFuser">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/cage/"><img src="files/cage.gif" alt="CAGE: Controllable Articulation GEneration" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/cage/" class="paper-title">CAGE: Controllable Articulation GEneration</a><br>
      <a href="https://sevenljy.github.io/">Jiayi Liu</a>, <a href="https://iv-t.github.io/">Hou In (Ivan) Tam</a>, <a href="https://www.sfu.ca/~amahdavi/">Ali Mahdavi-Amiri</a>, Manolis Savva<br>
      CVPR 2024<br>
      <a href="https://arxiv.org/pdf/2312.09570">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/cage/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/hssd/"><img src="files/hssd.png" alt="Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/hssd/" class="paper-title">Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation</a><br>
      <a href="https://mukulkhanna.github.io/">Mukul Khanna</a>, <a href="https://sammaoys.github.io/">Yongsen Mao</a>, <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, <a href="https://www.sanjayharesh.com/">Sanjay Haresh</a>, <a href="https://cs.stanford.edu/~bps/">Brennan Shacklett</a>, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, <a href="https://scholar.google.com/citations?user=p463opcAAAAJ&hl=en">Alexander Clegg</a>, <a href="https://www.ericundersander.com/">Eric Undersander</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva<br>
      CVPR 2024<br>
      <a href="https://arxiv.org/pdf/2306.11290">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/hssd/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://ego-exo4d-data.org/"><img src="files/egoexo4d.png" alt="Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://ego-exo4d-data.org/" class="paper-title">Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</a><br>
      Kristen Grauman, ...<a href="https://www.sanjayharesh.com/">Sanjay Haresh</a>, ...<a href="https://sammaoys.github.io/">Yongsen Mao</a>, ...Manolis Savva, ...<br>
      CVPR 2024<br>
      <a href="https://arxiv.org/pdf/2311.18259">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://ego-exo4d-data.org/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/tt3dstar/"><img src="files/tt3dstar.png" alt="Text-to-3D Shape Generation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/tt3dstar/" class="paper-title">Text-to-3D Shape Generation</a><br>
      <a href="https://hanhung.github.io/">Han-Hung Lee</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a><br>
      Eurographics STAR (State of The Art Report), CGF 2024<br>
      <a href="https://arxiv.org/pdf/2403.13289">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/tt3dstar/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/generalizing_shape_retrieval/"><img src="files/genshaperetr.png" alt="Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/generalizing_shape_retrieval/" class="paper-title">Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects</a><br>
      <a href="https://ca.linkedin.com/in/qirui-wu-946450bb">Qirui Wu</a>, <a href="https://dritchie.github.io/">Daniel Ritchie</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a><br>
      3DV 2024<br>
      <a href="https://arxiv.org/pdf/2401.00405.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/generalizing_shape_retrieval/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/OPDMulti/"><img src="files/opdmulti.png" alt="OPDMulti: Openable Part Detection for Multiple Objects" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/OPDMulti/" class="paper-title">OPDMulti: Openable Part Detection for Multiple Objects</a><br>
      <a href="https://www.linkedin.com/in/xiaohao-sun-237537195">Xiaohao Sun</a>, <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a><br>
      3DV 2024<br>
      <a href="https://arxiv.org/pdf/2303.14087.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/OPDMulti/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/mopa/"><img src="files/mopa.gif" alt="MOPA: Modular Object Navigation with PointGoal Agents" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/mopa/" class="paper-title">MOPA: Modular Object Navigation with PointGoal Agents</a><br>
      <a href="https://www.linkedin.com/in/sonia-raychaudhuri/">Sonia Raychaudhuri</a>, <a href="https://www.tommasocampari.com/">Tommaso Campari</a>, <a href="https://unnat.github.io/">Unnat Jain</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a><br>
      WACV 2024<br>
      <a href="https://arxiv.org/pdf/2304.03696.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/mopa/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://ovmm.github.io/"><img src="files/homerobot.jpg" alt="HomeRobot: Open Vocabulary Mobile Manipulation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://ovmm.github.io/" class="paper-title">HomeRobot: Open Vocabulary Mobile Manipulation</a><br>
      <a href="https://yvsriram.github.io/">Sriram Yenamandra</a>, <a href="https://arunram.me/">Arun Ramachandran</a>, <a href="https://www.karmeshyadav.com/">Karmesh Yadav</a>, <a href="https://www.linkedin.com/in/austinspwang/">Austin Wang</a>, <a href="https://mukulkhanna.github.io/">Mukul Khanna</a>, <a href="https://theophilegervet.github.io/">Theo Gervet</a>, <a href="https://sites.google.com/view/tyjimmyyang">Jimmy (Tsung-Yen) Yang</a>, <a href="https://vidhijain.github.io/">Vidhi Jain</a>, <a href="https://scholar.google.com/citations?user=p463opcAAAAJ&hl=en">Alexander Clegg</a>, <a href="http://johnmturner.com/">John Turner</a>, <a href="https://www.cc.gatech.edu/~zk15/">Zsolt Kira</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="https://devendrachaplot.github.io/">Devendra Chaplot</a>, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, <a href="https://cs.stanford.edu/~roozbeh/">Roozbeh Mottaghi</a>, <a href="https://yonatanbisk.com/">Yonatan Bisk</a>, <a href="https://cpaxton.github.io/about/">Chris Paxton</a><br>
      CoRL 2023<br>
      <a href="https://arxiv.org/pdf/2306.11565.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://ovmm.github.io/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/paris/"><img src="files/paris.png" alt="PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/paris/" class="paper-title">PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects</a><br>
      <a href="https://sevenljy.github.io/">Jiayi Liu</a>, <a href="https://www.sfu.ca/~amahdavi/">Ali Mahdavi-Amiri</a>, Manolis Savva<br>
      ICCV 2023<br>
      <a href="https://3dlg-hcvc.github.io/paris/">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/paris/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://arxiv.org/abs/2304.03188"><img src="files/scene-survey.png" alt="Advances in Data-Driven Analysis and Synthesis of 3D Indoor Scenes" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://arxiv.org/abs/2304.03188" class="paper-title">Advances in Data-Driven Analysis and Synthesis of 3D Indoor Scenes</a><br>
      <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>, <a href="https://supriya-gdptl.github.io/">Supriya Gadi Patil</a>, <a href="https://manyili12345.github.io/">Manyi Li</a>, <a href="http://graphics.stanford.edu/~mdfisher/">Matthew Fisher</a>, Manolis Savva, <a href="https://www2.cs.sfu.ca/~haoz/">Hao Zhang</a><br>
      Computer Graphics Forum 2023 STAR (State of the Art Report)<br>
      <a href="https://arxiv.org/pdf/2304.03188">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://arxiv.org/abs/2304.03188">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://aihabitat.org/datasets/hm3d-semantics/"><img src="files/hm3dsem.png" alt="Habitat-Matterport 3D Semantics Dataset" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://aihabitat.org/datasets/hm3d-semantics/" class="paper-title">Habitat-Matterport 3D Semantics Dataset</a><br>
      <a href="https://www.karmeshyadav.com/">Karmesh Yadav</a>, <a href="https://ram81.github.io/">Ram Ramrakhya</a>, <a href="https://srama2512.github.io/">Santhosh Kumar Ramakrishnan</a>, <a href="https://theophilegervet.github.io/">Theo Gervet</a>, <a href="http://johnmturner.com/">John Turner</a>, <a href="https://skylion007.github.io/">Aaron Gokaslan</a>, <a href="https://www.linkedin.com/in/noah-maestre-20212081">Noah Maestre</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, Manolis Savva, <a href="https://scholar.google.com/citations?user=p463opcAAAAJ&hl=en">Alexander Clegg</a>, <a href="https://devendrachaplot.github.io/">Devendra Chaplot</a><br>
      CVPR 2023<br>
      <a href="https://arxiv.org/pdf/2210.05633.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://aihabitat.org/datasets/hm3d-semantics/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://github.com/reza-asad/3DSSR"><img src="files/3dssr.png" alt="3DSSR: 3D Subscene Retrieval" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://github.com/reza-asad/3DSSR" class="paper-title">3DSSR: 3D Subscene Retrieval</a><br>
      <a href="https://reza-asad.github.io/">Reza Asad</a>, Manolis Savva<br>
      CVPR 2023  Workshop on Structural and Compositional Learning on 3D Data<br>
      <a href="https://openaccess.thecvf.com/content/CVPR2023W/StruCo3D/papers/Asad_3DSSR_3D_Subscene_Retrieval_CVPRW_2023_paper.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://github.com/reza-asad/3DSSR">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://supriya-gdptl.github.io/papers/evaluate3d.html"><img src="files/3drotinv.png" alt="Evaluating 3D Shape Analysis Methods for Robustness to Rotation Invariance" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://supriya-gdptl.github.io/papers/evaluate3d.html" class="paper-title">Evaluating 3D Shape Analysis Methods for Robustness to Rotation Invariance</a><br>
      <a href="https://supriya-gdptl.github.io/">Supriya Gadi Patil</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva<br>
      CRV 2023<br>
      <a href="https://arxiv.org/abs/2305.18557">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://supriya-gdptl.github.io/papers/evaluate3d.html">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://wijmans.xyz/publication/eom/"><img src="files/eom.jpg" alt="Emergence of Maps in the Memories of Blind Navigation Agents" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://wijmans.xyz/publication/eom/" class="paper-title">Emergence of Maps in the Memories of Blind Navigation Agents</a><br>
      <a href="https://wijmans.xyz/">Erik Wijmans</a>, Manolis Savva, <a href="http://www.irfanessa.gatech.edu/">Irfan Essa</a>, <a href="http://web.engr.oregonstate.edu/~leestef/">Stefan Lee</a>, <a href="http://www.arimorcos.com/">Ari Morcos</a>, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a><br>
      ICLR 2023, Outstanding Paper Award<br>
      <a href="https://openreview.net/pdf?id=lTt4KjHSsyl">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://wijmans.xyz/publication/eom/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/multiscan/"><img src="files/multiscan.png" alt="MultiScan: Scalable RGBD scanning for 3D environments with articulated objects" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/multiscan/" class="paper-title">MultiScan: Scalable RGBD scanning for 3D environments with articulated objects</a><br>
      <a href="https://sammaoys.github.io/">Yongsen Mao</a>, <a href="https://scholar.google.ca/citations?user=scUaE38AAAAJ">Yiming Zhang</a>, <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva<br>
      NeurIPS 2022<br>
      <a href="https://openreview.net/pdf?id=YxUdazpgweG">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/multiscan/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/3dhoi/"><img src="files/3dhoi.png" alt="Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/3dhoi/" class="paper-title">Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges</a><br>
      <a href="https://www.sanjayharesh.com/">Sanjay Haresh</a>, <a href="https://www.linkedin.com/in/xiaohao-sun-237537195">Xiaohao Sun</a>, <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva<br>
      3DV 2022<br>
      <a href="https://arxiv.org/pdf/2209.05612.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/3dhoi/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/OPD/"><img src="files/opd.png" alt="OPD: Single-view 3D Openable Part Detection" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/OPD/" class="paper-title">OPD: Single-view 3D Openable Part Detection</a><br>
      <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, <a href="https://sammaoys.github.io/">Yongsen Mao</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a><br>
      ECCV 2022<br>
      <a href="https://arxiv.org/pdf/2203.16421.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/OPD/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://aihabitat.org/"><img src="files/habitat2.jpeg" alt="Habitat 2.0: Training Home Assistants to Rearrange their Habitat" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://aihabitat.org/" class="paper-title">Habitat 2.0: Training Home Assistants to Rearrange their Habitat</a><br>
      <a href="https://www.andrewszot.com/">Andrew Szot</a>, <a href="https://scholar.google.com/citations?user=p463opcAAAAJ&hl=en">Alexander Clegg</a>, <a href="https://www.ericundersander.com/">Eric Undersander</a>, <a href="https://wijmans.xyz/">Erik Wijmans</a>, <a href="http://www.yilizhao.net/">Yili Zhao</a>, <a href="http://johnmturner.com/">John Turner</a>, <a href="https://www.linkedin.com/in/noah-maestre-20212081">Noah Maestre</a>, <a href="http://www.mustafamukadam.com/">Mustafa Mukadam</a>, <a href="https://devendrachaplot.github.io/">Devendra Chaplot</a>, <a href="https://research.fb.com/people/maksymets-oleksandr/">Oleksandr Maksymets</a>, <a href="https://skylion007.github.io/">Aaron Gokaslan</a>, <a href="https://github.com/mosra">Vladimír Vondrus</a>, <a href="https://sameerdharur.github.io/">Sameer Dharur</a>, <a href="https://fmeier.github.io/">Franziska Meier</a>, <a href="https://www.linkedin.com/in/galuba">Wojciech Galuba</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="https://www.cc.gatech.edu/~zk15/">Zsolt Kira</a>, <a href="http://vladlen.info/">Vladlen Koltun</a>, <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>, Manolis Savva, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a><br>
      NeurIPS 2021<br>
      <a href="https://arxiv.org/pdf/2106.14405.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://aihabitat.org/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://aihabitat.org/datasets/hm3d/"><img src="files/hm3d.jpg" alt="Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://aihabitat.org/datasets/hm3d/" class="paper-title">Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI</a><br>
      <a href="https://srama2512.github.io/">Santhosh Kumar Ramakrishnan</a>, <a href="https://skylion007.github.io/">Aaron Gokaslan</a>, <a href="https://wijmans.xyz/">Erik Wijmans</a>, <a href="https://research.fb.com/people/maksymets-oleksandr/">Oleksandr Maksymets</a>, <a href="https://scholar.google.com/citations?user=p463opcAAAAJ&hl=en">Alexander Clegg</a>, <a href="http://johnmturner.com/">John Turner</a>, <a href="https://www.ericundersander.com/">Eric Undersander</a>, <a href="https://www.linkedin.com/in/galuba">Wojciech Galuba</a>, <a href="https://www.linkedin.com/in/andrew-westbury-2757a31b">Andrew Westbury</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="http://www.yilizhao.net/">Yili Zhao</a>, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a><br>
      NeurIPS 2021 Datasets & Benchmarks Track<br>
      <a href="https://arxiv.org/pdf/2109.08238.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://aihabitat.org/datasets/hm3d/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://shivanshpatel35.github.io/comon/"><img src="files/comon.png" alt="Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://shivanshpatel35.github.io/comon/" class="paper-title">Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents</a><br>
      <a href="https://shivanshpatel35.github.io/">Shivansh Patel</a>, <a href="https://saimwani.github.io/">Saim Wani</a>, <a href="https://unnat.github.io/">Unnat Jain</a>, <a href="https://alexander-schwing.de/">Alexander Schwing</a>, <a href="https://slazebni.cs.illinois.edu/">Svetlana Lazebnik</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a><br>
      ICCV 2021<br>
      <a href="https://arxiv.org/abs/2110.05769">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://shivanshpatel35.github.io/comon/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://github.com/facebookresearch/d3d-hoi"><img src="files/d3dhoi.jpg" alt="D3D-HOI: Dynamic 3D Human-Object Interactions from Videos" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://github.com/facebookresearch/d3d-hoi" class="paper-title">D3D-HOI: Dynamic 3D Human-Object Interactions from Videos</a><br>
      <a href="https://samxuxiang.github.io/">Xiang Xu</a>, <a href="https://jhugestar.github.io/">Hanbyul Joo</a>, <a href="https://www.cs.sfu.ca/~mori/">Greg Mori</a>, Manolis Savva<br>
      arXiv:2108.08420 [cs.CV]<br>
      <a href="https://arxiv.org/abs/2108.08420">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://github.com/facebookresearch/d3d-hoi">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://www.youtube.com/watch?v=GBzpgLPP5VY&amp;ab_channel=KaiWang"><img src="files/roominoes.png" alt="Roominoes: Learning to Assemble 3D Rooms into Floor Plans" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://www.youtube.com/watch?v=GBzpgLPP5VY&amp;ab_channel=KaiWang" class="paper-title">Roominoes: Learning to Assemble 3D Rooms into Floor Plans</a><br>
      <a href="https://kwang-ether.github.io">Kai Wang</a>, <a href="https://www.linkedin.com/in/xianghao-xu-8b1024a6/">Xianghao Xu</a>, <a href="https://leonlei.com/portfolio/">Leon Lei</a>, <a href="https://www.selenaling.com/">Selena Ling</a>, <a href="https://www.linkedin.com/in/natalie-lindsay-660518109">Natalie Lindsay</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="https://dritchie.github.io/">Daniel Ritchie</a><br>
      SGP 2021<br>
      <a href="https://drive.google.com/file/d/1aSePGciFxnLRF4dhJynmNNctVfndIRpE/view">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://www.youtube.com/watch?v=GBzpgLPP5VY&amp;ab_channel=KaiWang">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/plan2scene/"><img src="files/plan2scene.png" alt="Plan2Scene: Converting Floorplans to 3D Scenes" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/plan2scene/" class="paper-title">Plan2Scene: Converting Floorplans to 3D Scenes</a><br>
      <a href="https://madhawav.github.io/">Madhawa Vidanapathirana</a>, <a href="https://ca.linkedin.com/in/qirui-wu-946450bb">Qirui Wu</a>, <a href="https://www2.cs.sfu.ca/~furukawa/">Yasutaka Furukawa</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva<br>
      CVPR 2021<br>
      <a href="https://arxiv.org/pdf/2106.05375.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/plan2scene/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://3dlg-hcvc.github.io/mirror3d/"><img src="files/mirror3d.png" alt="Mirror3D: Depth Refinement for Mirror Surfaces" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://3dlg-hcvc.github.io/mirror3d/" class="paper-title">Mirror3D: Depth Refinement for Mirror Surfaces</a><br>
      <a href="https://christinatan0704.github.io/mysite/">Jiaqi Tan</a>, <a href="https://lewislinn.github.io/">Weijie (Lewis) Lin</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva<br>
      CVPR 2021<br>
      <a href="https://arxiv.org/pdf/2106.06629.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://3dlg-hcvc.github.io/mirror3d/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href=""><img src="files/layoutgmn.png" alt="LayoutGMN: Neural Graph Matching for Structural Layout Similarity" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="" class="paper-title">LayoutGMN: Neural Graph Matching for Structural Layout Similarity</a><br>
      <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>, <a href="https://manyili12345.github.io/">Manyi Li</a>, <a href="http://graphics.stanford.edu/~mdfisher/">Matthew Fisher</a>, Manolis Savva, <a href="https://www2.cs.sfu.ca/~haoz/">Hao Zhang</a><br>
      CVPR 2021<br>
      <a href="">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://openreview.net/forum?id=cP5IcoAkfKa"><img src="files/bps.png" alt="Large Batch Simulation for Deep Reinforcement Learning" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://openreview.net/forum?id=cP5IcoAkfKa" class="paper-title">Large Batch Simulation for Deep Reinforcement Learning</a><br>
      <a href="https://cs.stanford.edu/~bps/">Brennan Shacklett</a>, <a href="https://wijmans.xyz/">Erik Wijmans</a>, <a href="https://alex-petrenko.github.io/">Aleksei Petrenko</a>, Manolis Savva, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, <a href="http://vladlen.info/">Vladlen Koltun</a>, <a href="http://graphics.stanford.edu/~kayvonf/">Kayvon Fatahalian</a><br>
      International Conference on Learning Representations (ICLR), 2021<br>
      <a href="https://openreview.net/pdf?id=cP5IcoAkfKa">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://openreview.net/forum?id=cP5IcoAkfKa">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://arxiv.org/abs/2011.01975"><img src="files/rearrangement.png" alt="Rearrangement: A Challenge for Embodied AI" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://arxiv.org/abs/2011.01975" class="paper-title">Rearrangement: A Challenge for Embodied AI</a><br>
      <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="https://www.cc.gatech.edu/~chernova/">Sonia Chernova</a>, <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew J. Davison</a>, <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, <a href="http://vladlen.info/">Vladlen Koltun</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>, <a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ">Igor Mordatch</a>, <a href="https://cs.stanford.edu/~roozbeh/">Roozbeh Mottaghi</a>, Manolis Savva, <a href="http://ai.stanford.edu/~haosu/">Hao Su</a><br>
      arXiv:2011.01975 [cs.AI]<br>
      <a href="https://arxiv.org/pdf/2011.01975.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://arxiv.org/abs/2011.01975">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://shivanshpatel35.github.io/multi-ON/"><img src="files/multion.jpg" alt="Multi-ON: Benchmarking Semantic Map Memory using Multi-object Navigation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://shivanshpatel35.github.io/multi-ON/" class="paper-title">Multi-ON: Benchmarking Semantic Map Memory using Multi-object Navigation</a><br>
      <a href="https://saimwani.github.io/">Saim Wani</a>, <a href="https://shivanshpatel35.github.io/">Shivansh Patel</a>, <a href="https://unnat.github.io/">Unnat Jain</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva<br>
      NeurIPS 2020<br>
      <a href="https://arxiv.org/pdf/2012.03912.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://shivanshpatel35.github.io/multi-ON/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="http://visual.cs.brown.edu/projects/articulations-webpage/"><img src="files/maps.jpg" alt="Motion Annotation Programs: A Scalable Approach to Annotating Kinematic Articulations in Large 3D Shape Collections" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="http://visual.cs.brown.edu/projects/articulations-webpage/" class="paper-title">Motion Annotation Programs: A Scalable Approach to Annotating Kinematic Articulations in Large 3D Shape Collections</a><br>
      <a href="https://www.linkedin.com/in/xianghao-xu-8b1024a6/">Xianghao Xu</a>, <a href="http://davidcharatan.com/">David Charatan</a>, <a href="https://www.linkedin.com/in/sonia-raychaudhuri/">Sonia Raychaudhuri</a>, <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, <a href="https://www.linkedin.com/in/mae-heitmann-770287118/">Mae Heitmann</a>, <a href="http://www.vovakim.com/">Vladimir Kim</a>, <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="https://dritchie.github.io/">Daniel Ritchie</a><br>
      3DV 2020<br>
      <a href="http://visual.cs.brown.edu/projects/articulations-webpage/articulations_3dv2020.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="http://visual.cs.brown.edu/projects/articulations-webpage/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://github.com/ChanganVR/RelationalGraphLearning"><img src="files/rgl.jpg" alt="Relational Graph Learning for Crowd Navigation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://github.com/ChanganVR/RelationalGraphLearning" class="paper-title">Relational Graph Learning for Crowd Navigation</a><br>
      <a href="https://changan.io/">Changan Chen</a>, <a href="https://husha1993.github.io/">Sha Hu</a>, <a href="http://payamn.github.io/">Payam Nikdel</a>, <a href="https://www.cs.sfu.ca/~mori/">Greg Mori</a>, Manolis Savva<br>
      International Conference on Intelligent Robots and Systems (IROS) 2020<br>
      <a href="https://arxiv.org/pdf/1909.13165.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://github.com/ChanganVR/RelationalGraphLearning">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://abhiskk.github.io/sim2real"><img src="files/srcc.jpg" alt="Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://abhiskk.github.io/sim2real" class="paper-title">Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation</a><br>
      <a href="https://abhiskk.github.io/">Abhishek Kadian</a>, <a href="https://www.joannetruong.com/">Joanne Truong</a>, <a href="https://skylion007.github.io/">Aaron Gokaslan</a>, <a href="https://scholar.google.com/citations?user=p463opcAAAAJ&hl=en">Alexander Clegg</a>, <a href="https://wijmans.xyz/">Erik Wijmans</a>, <a href="http://web.engr.oregonstate.edu/~leestef/">Stefan Lee</a>, Manolis Savva, <a href="https://www.cc.gatech.edu/~chernova/">Sonia Chernova</a>, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a><br>
      Robotics and Automation Letters (RA-L) & IROS 2020<br>
      <a href="https://arxiv.org/pdf/1912.06321.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://abhiskk.github.io/sim2real">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://github.com/facebookresearch/habitat-api"><img src="files/ddppo.jpg" alt="DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://github.com/facebookresearch/habitat-api" class="paper-title">DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames</a><br>
      <a href="https://wijmans.xyz/">Erik Wijmans</a>, <a href="https://abhiskk.github.io/">Abhishek Kadian</a>, <a href="http://www.arimorcos.com/">Ari Morcos</a>, <a href="http://web.engr.oregonstate.edu/~leestef/">Stefan Lee</a>, <a href="http://www.irfanessa.gatech.edu/">Irfan Essa</a>, <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>, Manolis Savva, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a><br>
      International Conference on Learning Representations (ICLR), 2020<br>
      <a href="https://arxiv.org/pdf/1911.00357.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://github.com/facebookresearch/habitat-api">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://github.com/brownvc/planit"><img src="files/graphsynth.png" alt="PlanIT: Planning and Instantiating Indoor Scenes with Relation Graph and Spatial Prior Networks" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://github.com/brownvc/planit" class="paper-title">PlanIT: Planning and Instantiating Indoor Scenes with Relation Graph and Spatial Prior Networks</a><br>
      <a href="https://kwang-ether.github.io">Kai Wang</a>, Yu-An Lin, Ben Weissmann, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="https://dritchie.github.io/">Daniel Ritchie</a><br>
      SIGGRAPH 2019<br>
      <a href="https://drive.google.com/file/d/1CJCM6EQyeUWwxdk6tl8cVxEIhV7s3DoA/view?usp=sharing">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://github.com/brownvc/planit">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://github.com/facebookresearch/Replica-Dataset"><img src="files/replica.png" alt="The Replica Dataset: A Digital Replica of Indoor Spaces" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://github.com/facebookresearch/Replica-Dataset" class="paper-title">The Replica Dataset: A Digital Replica of Indoor Spaces</a><br>
      <a href="http://people.csail.mit.edu/jstraub/">Julian Straub</a>, Thomas Whelan, Lingni Ma, Yufan Chen, <a href="https://wijmans.xyz/">Erik Wijmans</a>, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, Richard Newcombe<br>
      arXiv:1906.05797 [cs.CV]<br>
      <a href="https://arxiv.org/pdf/1906.05797.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://github.com/facebookresearch/Replica-Dataset">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://aihabitat.org/"><img src="files/habitat.png" alt="Habitat: A Platform for Embodied AI Research" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://aihabitat.org/" class="paper-title">Habitat: A Platform for Embodied AI Research</a><br>
      Manolis Savva, <a href="https://abhiskk.github.io/">Abhishek Kadian</a>, <a href="https://research.fb.com/people/maksymets-oleksandr/">Oleksandr Maksymets</a>, <a href="http://www.yilizhao.net/">Yili Zhao</a>, <a href="https://wijmans.xyz/">Erik Wijmans</a>, <a href="https://github.com/bhavanajain">Bhavana Jain</a>, <a href="http://people.csail.mit.edu/jstraub/">Julian Straub</a>, <a href="https://www.linkedin.com/in/jia-liu-a1221a2">Jia Liu</a>, <a href="http://vladlen.info/">Vladlen Koltun</a>, <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>, <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>, <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a><br>
      ICCV 2019, Best Paper Award Nominee<br>
      <a href="https://arxiv.org/pdf/1904.01201.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://aihabitat.org/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="http://www.yifeishi.net/hierarchylayout.html"><img src="files/hierarchylayout1.JPG" alt="Hierarchy Denoising Recursive Autoencoders for 3D Scene Layout Prediction" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="http://www.yifeishi.net/hierarchylayout.html" class="paper-title">Hierarchy Denoising Recursive Autoencoders for 3D Scene Layout Prediction</a><br>
      <a href="http://www.yifeishi.net/">Yifei Shi</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Zhelun Wu, Manolis Savva, <a href="https://kevinkaixu.net/">Kai Xu</a><br>
      CVPR 2019, arXiv:1903.03757 [cs.CV]<br>
      <a href="https://arxiv.org/pdf/1903.03757">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="http://www.yifeishi.net/hierarchylayout.html">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://niessnerlab.org/projects/avetisyan2019scan2cad.html"><img src="files/scan2cad.jpg" alt="Scan2CAD: Learning CAD Model Alignment in RGB-D Scans" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://niessnerlab.org/projects/avetisyan2019scan2cad.html" class="paper-title">Scan2CAD: Learning CAD Model Alignment in RGB-D Scans</a><br>
      <a href="https://niessnerlab.org/members/armen_avetisyan/profile.html">Armen Avetisyan</a>, <a href="https://niessnerlab.org/members/manuel_dahnert/profile.html">Manuel Dahnert</a>, <a href="http://graphics.stanford.edu/~adai/">Angela Dai</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://graphics.stanford.edu/~niessner/">Matthias Nießner</a><br>
      CVPR 2019, arXiv:1811.11187 [cs.CV]<br>
      <a href="https://arxiv.org/pdf/1811.11187.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://www.youtube.com/watch?v=PiHSYpgLTfA">video</a> 
      
       | <a href="https://niessnerlab.org/projects/avetisyan2019scan2cad.html">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://kwang-ether.github.io/deep-synth/"><img src="files/deepsynth.png" alt="Deep Convolutional Priors for Indoor Scene Synthesis" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://kwang-ether.github.io/deep-synth/" class="paper-title">Deep Convolutional Priors for Indoor Scene Synthesis</a><br>
      <a href="https://kwang-ether.github.io">Kai Wang</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="https://dritchie.github.io/">Daniel Ritchie</a><br>
      SIGGRAPH 2018<br>
      <a href="files/deepsynth.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://kwang-ether.github.io/deep-synth/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://arxiv.org/pdf/1807.06757.pdf"><img src="files/spl.jpg" alt="On Evaluation of Embodied Navigation Agents" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://arxiv.org/pdf/1807.06757.pdf" class="paper-title">On Evaluation of Embodied Navigation Agents</a><br>
      <a href="https://www.panderson.me/">Peter Anderson</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://www.cs.cmu.edu/~dchaplot/">Devendra Singh Chaplot</a>, <a href="https://dosovits.github.io/">Alexey Dosovitskiy</a>, <a href="http://saurabhg.web.illinois.edu/">Saurabh Gupta</a>, <a href="http://vladlen.info/">Vladlen Koltun</a>, <a href="https://cs.gmu.edu/~kosecka/">Jana Kosecka</a>, <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>, <a href="https://cs.stanford.edu/~roozbeh/">Roozbeh Mottaghi</a>, Manolis Savva, <a href="https://cs.stanford.edu/~amirz/">Amir R. Zamir</a><br>
      arXiv:1807.06757 [cs.AI]<br>
      <a href="https://arxiv.org/abs/1807.06757">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://arxiv.org/pdf/1807.06757.pdf">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="files/func_star.pdf"><img src="files/func_star.jpg" alt="Functionality Representations and Applications for Shape Analysis" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="files/func_star.pdf" class="paper-title">Functionality Representations and Applications for Shape Analysis</a><br>
      <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, Manolis Savva, <a href="http://people.scs.carleton.ca/~olivervankaick/">Oliver van Kaick</a><br>
      Eurographics STAR (State of The Art Report), CGF 2018<br>
      <a href="files/func_star.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="files/func_star.pdf">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://text2shape.stanford.edu"><img src="files/text2shape.png" alt="Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://text2shape.stanford.edu" class="paper-title">Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings</a><br>
      Kevin Chen, <a href="https://chrischoy.github.io/">Christopher B. Choy</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>, <a href="http://cvgl.stanford.edu/silvio/">Silvio Savarese</a><br>
      arXiv:1803.08495 [cs.CV]<br>
      <a href="https://arxiv.org/pdf/1803.08495">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://text2shape.stanford.edu">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://www.minosworld.org/"><img src="https://github.com/minosworld/minos/raw/master/docs/img/video_thumbnail.png" alt="MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://www.minosworld.org/" class="paper-title">MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments</a><br>
      Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="https://dosovits.github.io/">Alexey Dosovitskiy</a>, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>, <a href="http://vladlen.info/">Vladlen Koltun</a><br>
      arXiv:1712.03931 [cs.LG]<br>
      <a href="https://arxiv.org/abs/1712.03931">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://www.minosworld.org/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://im2pano3d.cs.princeton.edu/"><img src="https://im2pano3d.cs.princeton.edu/teaser.jpg" alt="Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://im2pano3d.cs.princeton.edu/" class="paper-title">Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View</a><br>
      <a href="http://vision.princeton.edu/people/shurans/">Shuran Song</a>, <a href="http://www.cs.princeton.edu/~andyz/">Andy Zeng</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="http://cvgl.stanford.edu/silvio/">Silvio Savarese</a>, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a><br>
      CVPR 2018, arXiv:1712.04569 [cs.CV]<br>
      <a href="https://arxiv.org/abs/1712.04569">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://im2pano3d.cs.princeton.edu/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://compling.hss.ntu.edu.sg/events/2018-gwc/pdfs/GWC2018_paper_66.pdf"><img src="files/wordnetLink.png" alt="Linking WordNet to 3D Shapes" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="" class="paper-title">Linking WordNet to 3D Shapes</a><br>
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Rishi Mago, Pranav Krishna, Manolis Savva, <a href="https://www.cs.princeton.edu/~fellbaum/">Christiane Fellbaum</a><br>
      Proceedings of Global WordNet Conference 2018<br>
      <a href="https://compling.hss.ntu.edu.sg/events/2018-gwc/pdfs/GWC2018_paper_66.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://cseweb.ucsd.edu/~haosu/papers/3dv2017_attribute_transfer.pdf"><img src="files/attributeTransfer.png" alt="Cross-modal Attribute Transfer for Rescaling 3D Models" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="" class="paper-title">Cross-modal Attribute Transfer for Rescaling 3D Models</a><br>
      <a href="https://icme.stanford.edu/people/lin-shao">Lin Shao</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://ai.stanford.edu/~haosu/">Hao Su</a>, Manolis Savva, <a href="http://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a><br>
      3DV 2017<br>
      <a href="https://cseweb.ucsd.edu/~haosu/papers/3dv2017_attribute_transfer.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://niessner.github.io/Matterport/"><img src="files/matterport3d.png" alt="Matterport3D: Learning from RGB-D Data in Indoor Environments" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://niessner.github.io/Matterport/" class="paper-title">Matterport3D: Learning from RGB-D Data in Indoor Environments</a><br>
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://graphics.stanford.edu/~adai/">Angela Dai</a>, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>, <a href="http://www.cs.princeton.edu/~mhalber/">Maciej Halber</a>, <a href="http://graphics.stanford.edu/~niessner/">Matthias Nießner</a>, Manolis Savva, <a href="http://vision.princeton.edu/people/shurans/">Shuran Song</a>, <a href="http://www.cs.princeton.edu/~andyz/">Andy Zeng</a>, <a href="http://robots.princeton.edu/people/yindaz/">Yinda Zhang</a><br>
      3DV 2017<br>
      <a href="https://arxiv.org/pdf/1709.06158">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://arxiv.org/abs/1709.06158">bib</a> 
      
      
      
       | <a href="https://niessner.github.io/Matterport/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://arxiv.org/abs/1704.02393"><img src="files/viewsets.png" alt="Learning Where to Look: Data-Driven Viewpoint Set Selection for 3D Scenes" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://arxiv.org/abs/1704.02393" class="paper-title">Learning Where to Look: Data-Driven Viewpoint Set Selection for 3D Scenes</a><br>
      <a href="http://3dvision.princeton.edu/people.html">Kyle Genova</a>, Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a><br>
      arXiv:1702.04405 [cs.CV]<br>
      <a href="https://arxiv.org/pdf/1704.02393">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://arxiv.org/abs/1704.02393">bib</a> 
      
      
      
       | <a href="https://arxiv.org/abs/1704.02393">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://www.scan-net.org/"><img src="files/scannet.jpg" alt="ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://www.scan-net.org/" class="paper-title">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</a><br>
      <a href="http://graphics.stanford.edu/~adai/">Angela Dai</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="http://www.cs.princeton.edu/~mhalber/">Maciej Halber</a>, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>, <a href="http://graphics.stanford.edu/~niessner/">Matthias Nießner</a><br>
      CVPR 2017, arXiv:1702.04405 [cs.CV]<br>
      <a href="https://arxiv.org/pdf/1702.04405">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://www.scan-net.org/">bib</a> 
      
      
      
       | <a href="https://www.scan-net.org/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://robots.princeton.edu/projects/2016/PBRS/"><img src="files/pbrs.jpg" alt="Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://robots.princeton.edu/projects/2016/PBRS/" class="paper-title">Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks</a><br>
      <a href="http://robots.princeton.edu/people/yindaz/">Yinda Zhang</a>, <a href="http://vision.princeton.edu/people/shurans/">Shuran Song</a>, <a href="http://www.meyumer.com/">Ersin Yumer</a>, Manolis Savva, <a href="https://sites.google.com/site/jyleecv/">Joon-Young Lee</a>, <a href="https://research.adobe.com/person/hailin-jin/">Hailin Jin</a>, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a><br>
      CVPR 2017, arXiv:1612.07429 [cs.CV]<br>
      <a href="https://arxiv.org/pdf/1612.07429v2.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://robots.princeton.edu/projects/2016/PBRS/">bib</a> 
      
      
      
       | <a href="https://robots.princeton.edu/projects/2016/PBRS/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://sscnet.cs.princeton.edu/"><img src="files/sscnet.jpg" alt="Semantic Scene Completion from a Single Depth Image" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://sscnet.cs.princeton.edu/" class="paper-title">Semantic Scene Completion from a Single Depth Image</a><br>
      <a href="http://vision.princeton.edu/people/shurans/">Shuran Song</a>, <a href="http://yf.io/">Fisher Yu</a>, <a href="http://www.cs.princeton.edu/~andyz/">Andy Zeng</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a><br>
      CVPR 2017, arXiv:1611.08974 [cs.CV]<br>
      <a href="https://arxiv.org/pdf/1611.08974v1.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://sscnet.cs.princeton.edu/bibtex.bib">bib</a> 
      
      
      
       | <a href="https://sscnet.cs.princeton.edu/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://arxiv.org/abs/1703.00061"><img src="files/scenesuggest.png" alt="SceneSuggest: Context-driven 3D Scene Design" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://arxiv.org/abs/1703.00061" class="paper-title">SceneSuggest: Context-driven 3D Scene Design</a><br>
      Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://vis.berkeley.edu/~maneesh">Maneesh Agrawala</a><br>
      arXiv:1703.00061 [cs.GR]<br>
      <a href="https://arxiv.org/pdf/1703.00061.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://arxiv.org/abs/1703.00061">bib</a> 
      
      
      
       | <a href="https://arxiv.org/abs/1703.00061">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://arxiv.org/abs/1703.00050"><img src="files/sceneseer.png" alt="SceneSeer: 3D Scene Design with Natural Language" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://arxiv.org/abs/1703.00050" class="paper-title">SceneSeer: 3D Scene Design with Natural Language</a><br>
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://stanford.edu/~meric/">Mihail Eric</a>, Manolis Savva, <a href="http://nlp.stanford.edu/manning/">Christopher D. Manning</a><br>
      arXiv:1703.00050 [cs.GR]<br>
      <a href="https://arxiv.org/pdf/1703.00050.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://arxiv.org/abs/1703.00050">bib</a> 
      
      
      
       | <a href="https://arxiv.org/abs/1703.00050">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://vcc.szu.edu.cn/courses/functionality/"><img src="files/shape-functionality-course.png" alt="Directions in Shape Analysis towards Functionality" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://vcc.szu.edu.cn/courses/functionality/" class="paper-title">Directions in Shape Analysis towards Functionality</a><br>
      <a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, <a href="http://people.scs.carleton.ca/~olivervankaick/">Oliver van Kaick</a>, <a href="http://sist.shanghaitech.edu.cn/faculty/zhengyy/">Youyi Zheng</a>, Manolis Savva<br>
      SIGGRAPH Asia Course, 2016<br>
      <a href="https://vcc.szu.edu.cn/courses/functionality/">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://vcc.szu.edu.cn/courses/functionality/">bib</a> 
      
      
      
       | <a href="https://vcc.szu.edu.cn/courses/functionality/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://purl.stanford.edu/db690fq5772"><img src="files/thesis.png" alt="Body-centric Understanding of 3D Environments" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://purl.stanford.edu/db690fq5772" class="paper-title">Body-centric Understanding of 3D Environments</a><br>
      Manolis Savva<br>
      Ph.D. dissertation, Department of Computer Science, Stanford University, 2016<br>
      <a href="https://purl.stanford.edu/db690fq5772">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://purl.stanford.edu/db690fq5772">bib</a> 
      
      
      
       | <a href="https://purl.stanford.edu/db690fq5772">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://graphics.stanford.edu/projects/pigraphs/"><img src="https://graphics.stanford.edu/projects/pigraphs/pigraphs.png" alt="PiGraphs: Learning Interaction Snapshots from Observations" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://graphics.stanford.edu/projects/pigraphs/" class="paper-title">PiGraphs: Learning Interaction Snapshots from Observations</a><br>
      Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a>, <a href="http://graphics.stanford.edu/~mdfisher/">Matthew Fisher</a>, <a href="http://graphics.stanford.edu/~niessner/">Matthias Nießner</a><br>
      SIGGRAPH 2016 Technical Papers<br>
      <a href="https://graphics.stanford.edu/projects/pigraphs/">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://graphics.stanford.edu/projects/pigraphs/">bib</a> 
      
      
      
       | <a href="https://graphics.stanford.edu/projects/pigraphs/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://shapenet.cs.stanford.edu/shrec16/"><img src="files/shrec16shapenet.png" alt="SHREC'16 Track: Large-Scale 3D Shape Retrieval from ShapeNet Core55" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://shapenet.cs.stanford.edu/shrec16/" class="paper-title">SHREC'16 Track: Large-Scale 3D Shape Retrieval from ShapeNet Core55</a><br>
      Manolis Savva, Fisher Yu, Hao Su, Masaki Aono, Baoquan Chen, Daniel Cohen-Or, Weihong Deng, Hang Su, Song Bai, Xiang Bai, Noa Fish, Jiajie Han, Evangelos Kalogerakis, Erik G. Learned-Miller, Yangyan Li, Minghui Liao, Subhransu Maji, Atsushi Tatsuma, Yida Wang, Nanhai Zhang, Zhichao Zhou<br>
      Eurographics Workshop on 3D Object Retrieval 2016<br>
      <a href="https://shapenet.cs.stanford.edu/shrec16/shrec16shapenet.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://shapenet.cs.stanford.edu/shrec16/">bib</a> 
      
      
      
       | <a href="https://shapenet.cs.stanford.edu/shrec16/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://arxiv.org/abs/1512.03012"><img src="files/shapenet.png" alt="ShapeNet: An Information-Rich 3D Model Repository" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://arxiv.org/abs/1512.03012" class="paper-title">ShapeNet: An Information-Rich 3D Model Repository</a><br>
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>, <a href="http://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a>, <a href="http://ttic.uchicago.edu/~huangqx/">Qixing Huang</a>, Zimo Li, <a href="http://cvgl.stanford.edu/silvio/">Silvio Savarese</a>, Manolis Savva, <a href="http://vision.princeton.edu/people/shurans/">Shuran Song</a>, <a href="http://ai.stanford.edu/~haosu/">Hao Su</a>, <a href="http://vision.princeton.edu/people/xj/">Jianxiong Xiao</a>, <a href="http://geometry.stanford.edu/person.php?id=ericyi">Li Yi</a>, <a href="http://yf.io/">Fisher Yu</a><br>
      arXiv:1512.03012 [cs.GR], Dec 2015<br>
      <a href="https://arxiv.org/pdf/1512.03012v1">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://shapenet.cs.stanford.edu/resources/shapenet.bib">bib</a> 
      
      
      
       | <a href="https://arxiv.org/abs/1512.03012">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://graphics.stanford.edu/projects/actsynth/"><img src="files/actsynth.png" alt="Activity-centric Scene Synthesis for Functional 3D Scene Modeling" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://graphics.stanford.edu/projects/actsynth/" class="paper-title">Activity-centric Scene Synthesis for Functional 3D Scene Modeling</a><br>
      <a href="http://graphics.stanford.edu/~mdfisher/">Matthew Fisher</a>, Manolis Savva, <a href="http://web.stanford.edu/~yangyan/">Yangyan Li</a>, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a>, <a href="http://graphics.stanford.edu/~niessner/">Matthias Nießner</a><br>
      SIGGRAPH Asia 2015 Technical Papers<br>
      <a href="https://graphics.stanford.edu/~niessner/papers/2015/9synth/fisher2015activity_orig.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://graphics.stanford.edu/projects/actsynth/fisher2015actsynth.bib">bib</a> 
      
      
      
       | <a href="https://graphics.stanford.edu/projects/actsynth/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://graphics.stanford.edu/projects/semgeo/"><img src="files/semgeo.png" alt="Semantically-Enriched 3D Models for Common-sense Knowledge" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://graphics.stanford.edu/projects/semgeo/" class="paper-title">Semantically-Enriched 3D Models for Common-sense Knowledge</a><br>
      Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a><br>
      CVPR 2015 Vision meets Cognition Workshop<br>
      <a href="https://graphics.stanford.edu/projects/semgeo/semgeo.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://graphics.stanford.edu/projects/semgeo/semgeo.bib">bib</a> 
      
      
      
       | <a href="https://graphics.stanford.edu/projects/semgeo/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://nlp.stanford.edu/data/text2scene.shtml"><img src="files/lexground.png" alt="Text to 3D Scene Generation with Rich Lexical Grounding" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://nlp.stanford.edu/data/text2scene.shtml" class="paper-title">Text to 3D Scene Generation with Rich Lexical Grounding</a><br>
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://www.stanford.edu/~wmonroe4/">Will Monroe</a>, Manolis Savva, <a href="http://web.stanford.edu/~cgpotts/">Christopher Potts</a>, <a href="http://nlp.stanford.edu/manning/">Christopher D. Manning</a><br>
      Proceedings of ACL 2015<br>
      <a href="https://nlp.stanford.edu/pubs/chang-acl2015-lexground.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://nlp.stanford.edu/pubs/chang-acl2015-lexground.bib">bib</a> 
      
      
      
       | <a href="https://nlp.stanford.edu/data/text2scene.shtml">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://graphics.stanford.edu/projects/scenegrok/"><img src="files/scenegrok.png" alt="SceneGrok: Inferring Action Maps in 3D Environments" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://graphics.stanford.edu/projects/scenegrok/" class="paper-title">SceneGrok: Inferring Action Maps in 3D Environments</a><br>
      Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a>, <a href="http://graphics.stanford.edu/~mdfisher/">Matthew Fisher</a>, <a href="http://graphics.stanford.edu/~niessner/">Matthias Nießner</a><br>
      SIGGRAPH Asia 2014 Technical Papers<br>
      <a href="https://graphics.stanford.edu/projects/scenegrok/scenegrok.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://graphics.stanford.edu/projects/scenegrok/scenegrok.bib">bib</a> 
      
      
      
       | <a href="https://graphics.stanford.edu/projects/scenegrok/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://graphics.stanford.edu/projects/sizes/"><img src="files/sizes.png" alt="On Being the Right Scale: Sizing Large Collections of 3D Models" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://graphics.stanford.edu/projects/sizes/" class="paper-title">On Being the Right Scale: Sizing Large Collections of 3D Models</a><br>
      Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://graphics.stanford.edu/~gilbo/">Gilbert Bernstein</a>, <a href="http://nlp.stanford.edu/manning/">Christopher D. Manning</a>, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a><br>
      SIGGRAPH Asia 2014 Workshop on Indoor Scene Understanding: Where Graphics meets Vision<br>
      <a href="https://graphics.stanford.edu/projects/sizes/sizes.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://graphics.stanford.edu/projects/sizes/sizes.bib">bib</a> 
      
      
      
       | <a href="https://graphics.stanford.edu/projects/sizes/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://nlp.stanford.edu/pubs/spatial-emnlp2014.pdf"><img src="files/spatialLearning.png" alt="Learning Spatial Knowledge for Text to 3D Scene Generation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://nlp.stanford.edu/pubs/spatial-emnlp2014.pdf" class="paper-title">Learning Spatial Knowledge for Text to 3D Scene Generation</a><br>
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="http://nlp.stanford.edu/manning/">Christopher D. Manning</a><br>
      Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)<br>
      <a href="https://nlp.stanford.edu/pubs/spatial-emnlp2014.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://nlp.stanford.edu/pubs/spatial-emnlp2014.bib">bib</a> 
      
      
      
       | <a href="https://nlp.stanford.edu/pubs/spatial-emnlp2014.pdf">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="files/fpic2014.pdf"><img src="files/fpic2014.png" alt="Learning Affordance Maps by Observing Interactions" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="files/fpic2014.pdf" class="paper-title">Learning Affordance Maps by Observing Interactions</a><br>
      Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://graphics.stanford.edu/~mdfisher/">Matthew Fisher</a>, <a href="http://graphics.stanford.edu/~niessner/">Matthias Nießner</a>, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a><br>
      CVPR 2014 Workshop on Functionality, Physics, Intentionality and Causality<br>
      <a href="files/fpic2014.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="files/fpic2014.bib">bib</a> 
      
      
      
       | <a href="files/fpic2014.pdf">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://nlp.stanford.edu/pubs/scenegen-aclviz2014.pdf"><img src="files/interactiveLearning.png" alt="Interactive Learning of Spatial Knowledge for Text to 3D Scene Generation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://nlp.stanford.edu/pubs/scenegen-aclviz2014.pdf" class="paper-title">Interactive Learning of Spatial Knowledge for Text to 3D Scene Generation</a><br>
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="http://nlp.stanford.edu/manning/">Christopher D. Manning</a><br>
      Proceedings of the ACL 2014 Workshop on Interactive Language Learning, Visualization, and Interfaces<br>
      <a href="https://nlp.stanford.edu/pubs/scenegen-aclviz2014.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://nlp.stanford.edu/pubs/scenegen-aclviz2014.bib">bib</a> 
      
      
      
       | <a href="https://nlp.stanford.edu/pubs/scenegen-aclviz2014.pdf">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://nlp.stanford.edu/pubs/scenegen-sp2014.pdf"><img src="files/semanticParsing.png" alt="Semantic Parsing for Text to 3D Scene Generation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://nlp.stanford.edu/pubs/scenegen-sp2014.pdf" class="paper-title">Semantic Parsing for Text to 3D Scene Generation</a><br>
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, Manolis Savva, <a href="http://nlp.stanford.edu/manning/">Christopher D. Manning</a><br>
      Proceedings of the ACL 2014 Workshop on Semantic Parsing<br>
      <a href="https://nlp.stanford.edu/pubs/scenegen-sp2014.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://nlp.stanford.edu/pubs/scenegen-sp2014.bib">bib</a> 
      
      
      
       | <a href="https://nlp.stanford.edu/pubs/scenegen-sp2014.pdf">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://graphics.stanford.edu/projects/transphoner/"><img src="files/transphoner.png" alt="TransPhoner: Automated Mnemonic Keyword Generation" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://graphics.stanford.edu/projects/transphoner/" class="paper-title">TransPhoner: Automated Mnemonic Keyword Generation</a><br>
      Manolis Savva, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="http://nlp.stanford.edu/manning/">Christopher D. Manning</a>, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a><br>
      Proceedings of CHI 2014<br>
      <a href="https://graphics.stanford.edu/projects/transphoner/TransPhoner.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://graphics.stanford.edu/projects/transphoner/TransPhoner.bib">bib</a> 
      
      
      
       | <a href="https://graphics.stanford.edu/projects/transphoner/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://graphics.stanford.edu/projects/scenesynth/"><img src="files/sceneSynth.png" alt="Example-based Synthesis of 3D Object Arrangements" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://graphics.stanford.edu/projects/scenesynth/" class="paper-title">Example-based Synthesis of 3D Object Arrangements</a><br>
      <a href="http://graphics.stanford.edu/~mdfisher/">Matthew Fisher</a>, <a href="https://dritchie.github.io/">Daniel Ritchie</a>, Manolis Savva, <a href="http://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a><br>
      Proceedings of SIGGRAPH Asia 2012<br>
      <a href="https://graphics.stanford.edu/projects/scenesynth/scenesynth_paper.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://graphics.stanford.edu/projects/scenesynth/">bib</a> 
      
      
      
       | <a href="https://graphics.stanford.edu/projects/scenesynth/">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://vis.stanford.edu/papers/graphprism"><img src="files/graphprism.png" alt="GraphPrism: Compact Visualization of Network Structure" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://vis.stanford.edu/papers/graphprism" class="paper-title">GraphPrism: Compact Visualization of Network Structure</a><br>
      <a href="http://www.sanjaykairam.com/">Sanjay Kairam</a>, Diana MacLean, Manolis Savva, <a href="http://jheer.org/">Jeffrey Heer</a><br>
      Advanced Visual Interfaces, 2012<br>
      <a href="https://vis.stanford.edu/files/2012-GraphPrism-AVI.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://vis.stanford.edu/bibtex/graphprism">bib</a> 
      
      
      
       | <a href="https://vis.stanford.edu/papers/graphprism">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="https://vis.stanford.edu/papers/revision"><img src="files/revision.png" alt="ReVision: Automated Classification, Analysis and Redesign of Chart Images" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="https://vis.stanford.edu/papers/revision" class="paper-title">ReVision: Automated Classification, Analysis and Redesign of Chart Images</a><br>
      Manolis Savva, <a href="http://www.eecs.berkeley.edu/~nkong/">Nicholas Kong</a>, Arti Chhajta, <a href="http://vision.stanford.edu/">Li Fei-Fei</a>, <a href="http://vis.berkeley.edu/~maneesh">Maneesh Agrawala</a>, <a href="http://jheer.org/">Jeffrey Heer</a><br>
      ACM User Interface Software & Technology (UIST), 2011 <strong>[Notable Paper Award]</strong><br>
      <a href="https://vis.stanford.edu/files/2011-ReVision-UIST.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="https://vis.stanford.edu/bibtex/revision">bib</a> 
      
      
      
       | <a href="https://vis.stanford.edu/papers/revision">webpage</a> 
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 paper-img">
    
      <a href="files/graphkernel.pdf"><img src="files/graphkernel.png" alt="Characterizing Structural Relationships in Scenes using Graph Kernels" class="img-responsive"/></a>
    
    </div>
    <div class="col-xs-9 paper-text">
      <a href="files/graphkernel.pdf" class="paper-title">Characterizing Structural Relationships in Scenes using Graph Kernels</a><br>
      <a href="http://graphics.stanford.edu/~mdfisher/">Matthew Fisher</a>, Manolis Savva, <a href="http://graphics.stanford.edu/~hanrahan/">Pat Hanrahan</a><br>
      Proceedings of SIGGRAPH 2011<br>
      <a href="files/graphkernel.pdf">pdf</a>
      
      
      
      
      
      
      
      
      
      
      
      
      
       | <a href="files/graphkernel.bib">bib</a> 
      
      
      
       | <a href="files/graphkernel.pdf">webpage</a> 
    </div>
  </div>

  </div>
</div>
    </section>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
      var sc_project = 7724643;
      var sc_invisible = 1;
      var sc_security = "1e50ec33";
    </script>
    <script type="text/javascript"
            src="https://www.statcounter.com/counter/counter.js"></script>
    <noscript>
      <div class="statcounter"><a title="drupal stats"
                                  href="https://statcounter.com/drupal/" target="_blank"><img
          class="statcounter"
          src="https://c.statcounter.com/7724643/0/1e50ec33/0/"
          alt="drupal stats"></a></div>
    </noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <!-- <footer>
      <p class="pull-right">
        Last updated at 2025-07-21T03:14:49.004Z
      </p>
    </footer> -->
  </div><!-- /container -->
</body>
</html>
